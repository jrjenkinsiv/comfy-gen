# Agent Generation Guide

This guide is for AI agents and low-level workers to understand how to generate images and videos using ComfyGen.

## Table of Contents

- [Quick Reference](#quick-reference)
- [Decision Tree](#decision-tree)
- [Understanding Workflows](#understanding-workflows)
- [Available Models](#available-models)
- [Using LoRAs](#using-loras)
- [Prompt Engineering](#prompt-engineering)
- [Validation and Quality Control](#validation-and-quality-control)
- [Error Handling](#error-handling)
- [End-to-End Examples](#end-to-end-examples)

## Quick Reference

### Simple Image Generation (SD 1.5)

```bash
# From magneto or any machine with SSH access
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a red sports car on a mountain road, cinematic lighting" \
    --output /tmp/car.png
```

The image will be:
1. Generated by ComfyUI on moira
2. Saved locally to `/tmp/car.png`
3. Uploaded to MinIO at `http://192.168.1.215:9000/comfy-gen/<timestamp>_car.png`

### Viewing Generated Images

Images are viewable directly in browser (no download):
```
http://192.168.1.215:9000/comfy-gen/<filename>.png
```

List all images:
```bash
curl -s http://192.168.1.215:9000/comfy-gen/ | grep -oP '(?<=<Key>)[^<]+'
```

---

## Decision Tree

Use this flowchart to select the appropriate model and workflow:

```
START: User requests generation
    |
    ├─ Contains "video", "animation", "motion"?
    |   YES -> Is there an input image?
    |           YES -> Use Wan 2.2 I2V
    |           |      Workflow: workflows/wan22-i2v.json
    |           |      Model: wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors
    |           |      LoRA: wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
    |           |      Steps: 4, Duration: ~10 seconds
    |           |
    |           NO  -> Use Wan 2.2 T2V
    |                  Workflow: workflows/wan22-t2v.json
    |                  Model: wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors
    |                  LoRA: wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors
    |                  Steps: 4, Duration: ~10 seconds
    |
    ├─ Is there an input image to transform?
    |   YES -> Use SD 1.5 img2img
    |          Workflow: workflows/sd15-img2img.json
    |          Model: v1-5-pruned-emaonly-fp16.safetensors
    |          Denoise: 0.3-0.5 (subtle), 0.7-0.9 (creative)
    |          Duration: 10-30 seconds
    |
    NO -> Use SD 1.5 / Flux
          Workflow: workflows/flux-dev.json
          Model: v1-5-pruned-emaonly-fp16.safetensors
          Duration: 10-30 seconds

ADDITIONAL CONSIDERATIONS:
- Need quality validation? Add --validate --auto-retry
- Need specific physics/motion? Check LoRA catalog for enhancement LoRAs
- Fast generation priority? Ensure acceleration LoRAs are used (4-step)
```

### Decision Examples

**Request:** "Generate a video of a person walking"
- Contains "video" → YES
- Input image? → NO
- **Decision:** Wan 2.2 T2V, workflow: `wan22-t2v.json`

**Request:** "Transform this image to oil painting style"
- Video request? → NO
- Input image? → YES
- **Decision:** SD 1.5 img2img, workflow: `sd15-img2img.json`, denoise: 0.7

**Request:** "Create an image of a sunset"
- Video request? → NO
- Input image? → NO
- **Decision:** SD 1.5, workflow: `flux-dev.json`

**Request:** "Animate this photo with camera movement"
- Contains "animate" → YES (video)
- Input image? → YES
- **Decision:** Wan 2.2 I2V, workflow: `wan22-i2v.json`

---

## Understanding Workflows

A workflow is a JSON file that defines:
1. **What model to use** (checkpoint)
2. **What prompt to encode** (positive/negative)
3. **How to sample** (steps, CFG, sampler)
4. **What LoRAs to apply** (optional)

### Basic SD 1.5 Workflow Structure

```json
{
  "3": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "v1-5-pruned-emaonly-fp16.safetensors"
    }
  },
  "6": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "YOUR PROMPT HERE",
      "clip": ["3", 1]
    }
  },
  "7": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "bad quality, blurry",
      "clip": ["3", 1]
    }
  }
}
```

---

## Using LoRAs

LoRAs modify the model to achieve specific styles or effects. They are applied between the checkpoint loader and the sampler.

### LoraLoader Node Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | MODEL | - | Input from CheckpointLoader |
| `clip` | CLIP | - | Input from CheckpointLoader |
| `lora_name` | string | - | Filename of the LoRA |
| `strength_model` | float | 1.0 | How strongly to apply LoRA to model (-100 to 100) |
| `strength_clip` | float | 1.0 | How strongly to apply LoRA to CLIP (-100 to 100) |

### Example: Adding a LoRA to Workflow

```json
{
  "3": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "v1-5-pruned-emaonly-fp16.safetensors"
    }
  },
  "10": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["3", 0],
      "clip": ["3", 1],
      "lora_name": "example_lora.safetensors",
      "strength_model": 0.8,
      "strength_clip": 0.8
    }
  },
  "6": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "YOUR PROMPT",
      "clip": ["10", 1]
    }
  }
}
```

### Chaining Multiple LoRAs

```json
{
  "10": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["3", 0],
      "clip": ["3", 1],
      "lora_name": "first_lora.safetensors",
      "strength_model": 0.7,
      "strength_clip": 0.7
    }
  },
  "11": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["10", 0],
      "clip": ["10", 1],
      "lora_name": "second_lora.safetensors",
      "strength_model": 0.5,
      "strength_clip": 0.5
    }
  }
}
```

### LoRA Strength Guidelines

| Strength | Effect |
|----------|--------|
| 0.3-0.5 | Subtle influence |
| 0.6-0.8 | Moderate effect |
| 0.9-1.0 | Strong effect |
| 1.0+ | May cause artifacts |
| Negative | Inverse effect |

---

## Available Models

### Base Models (Checkpoints)

| Model | Filename | Best For |
|-------|----------|----------|
| SD 1.5 | `v1-5-pruned-emaonly-fp16.safetensors` | General images, fast |

### Video Models

| Model | Filename | Type |
|-------|----------|------|
| Wan 2.2 T2V High | `wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors` | Text-to-video |
| Wan 2.2 T2V Low | `wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors` | Text-to-video |

### Acceleration LoRAs (Reduce Steps)

These LoRAs allow generating in fewer steps:

| LoRA | Compatible With | Steps |
|------|----------------|-------|
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors` | Wan T2V High | 4 |
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors` | Wan T2V Low | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors` | Wan I2V High | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors` | Wan I2V Low | 4 |

---

## Decision Tree for Agents

### When to Use SD 1.5
- Simple image requests
- Fast generation needed
- Generic subjects (landscapes, objects, architecture)

### When to Use Wan 2.2
- Video generation requested
- Animation or motion needed
- Text-to-video: Use `workflows/wan22-t2v.json`
- Image-to-video: Use `workflows/wan22-i2v.json`
- Longer generation time acceptable (4 steps with acceleration LoRA)

### Choosing LoRA Strength
1. For acceleration LoRAs (4-step): Use `strength_model: 1.0, strength_clip: 1.0`
2. For style LoRAs: Start with `strength_model: 0.7, strength_clip: 0.7`
3. If effect too subtle, increase to 0.9
4. If artifacts appear, decrease to 0.5
5. For physics/motion LoRAs, use 0.6-0.8

---

## Error Handling

### ComfyUI Not Responding
```bash
# Check status
curl -s http://192.168.1.215:8188/system_stats

# If no response, start ComfyUI
ssh moira "C:\\Users\\jrjen\\comfy\\.venv\\Scripts\\python.exe C:\\Users\\jrjen\\comfy-gen\\scripts\\start_comfyui.py"
```

### Model Not Found
1. Check exact filename in MODEL_REGISTRY.md
2. Verify model exists: `ssh moira "dir C:\\Users\\jrjen\\comfy\\models\\checkpoints"`
3. Model names are case-sensitive

### LoRA Not Found
1. List available LoRAs via API:
   ```bash
   curl -s http://192.168.1.215:8188/object_info | python3 -c "
   import json,sys
   data=json.load(sys.stdin)
   for l in data['LoraLoader']['input']['required']['lora_name'][0]:
       print(l)
   "
   ```

---

## Prompt Engineering Tips

### For SD 1.5

**Good prompt structure:**
```
[subject], [style], [lighting], [quality modifiers]
```

**Example:**
```
a red Porsche 911 on a mountain road, cinematic photography, golden hour lighting, highly detailed, 8k
```

**Negative prompt (always include):**
```
bad quality, blurry, low resolution, watermark, text
```

### For Video (Wan 2.2)

**Good prompt structure:**
```
[action/motion], [subject], [setting], [camera movement]
```

**Example:**
```
a car driving along a coastal highway, waves crashing, drone shot following
```

---

## Workflow Templates

See `workflows/` directory for ready-to-use templates:

| Template | Use Case | Notes |
|----------|----------|-------|
| `flux-dev.json` | Simple SD 1.5 images | Fast, general purpose |
| `wan22-t2v.json` | Text-to-video | Uses Wan 2.2 with 4-step LoRA |
| `wan22-i2v.json` | Image-to-video | Animates existing images |

### Using Wan 2.2 Workflows

**Text-to-Video Example:**
```bash
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "a person walking through a park on a sunny day" \
    --output /tmp/video.mp4
```

**Image-to-Video Example:**
```bash
# Note: Requires modifying the workflow to specify input image
python3 generate.py \
    --workflow workflows/wan22-i2v.json \
    --prompt "the person starts walking forward" \
    --output /tmp/animated.mp4
```

**Wan 2.2 Video Specifications:**
- **Resolution:** 848x480 pixels
- **Frame Count:** 81 frames
- **Frame Rate:** 8 fps
- **Duration:** ~10 seconds
- **Steps:** 4 (with acceleration LoRA)
- **CFG:** 1.0 (recommended with 4-step LoRA)

To create a new workflow:
1. Export from ComfyUI GUI
2. Save to `workflows/` with descriptive name
3. Document in this file

---

## Validation and Quality Control

### Overview

ComfyGen includes automated validation to detect quality issues and optionally retry generation with adjusted prompts.

**Use validation when:**
- Generating subjects that often duplicate (e.g., cars, people)
- Quality consistency is critical
- You want to reduce manual iteration

### How Validation Works

1. **CLIP Scoring**: Computes semantic similarity between the generated image and your prompt
2. **Threshold Check**: Validates that the CLIP score meets minimum requirements
3. **Auto-Retry** (optional): Adjusts prompts and regenerates if validation fails

### Basic Usage

```bash
# Just validate (no retry)
python3 generate.py --workflow workflows/flux-dev.json \
    --prompt "a red Porsche 911" \
    --output /tmp/car.png \
    --validate

# Validate with auto-retry
python3 generate.py --workflow workflows/flux-dev.json \
    --prompt "(Porsche 911:2.0) single car, one car only" \
    --negative-prompt "multiple cars, duplicate" \
    --output /tmp/car.png \
    --validate --auto-retry --retry-limit 3
```

### Validation Options

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| `--validate` | Boolean | False | Run validation after generation |
| `--auto-retry` | Boolean | False | Retry if validation fails |
| `--retry-limit` | Integer | 3 | Maximum retry attempts |
| `--positive-threshold` | Float | 0.25 | Minimum CLIP score (0-1) |

### How Auto-Retry Adjusts Prompts

When validation fails and auto-retry is enabled:

1. **Positive prompt enhancement**:
   - Increases weight on key terms like "single car" → "(single car:1.3)"
   - Weight multiplier increases with each retry attempt

2. **Negative prompt strengthening**:
   - Adds terms: "multiple cars, duplicate, cloned, ghosting, mirrored"
   - Helps prevent common issues like duplicated subjects

### Validation Thresholds

**CLIP Score Interpretation:**
- **< 0.20**: Poor semantic match (likely wrong subject or major issues)
- **0.20-0.25**: Marginal match (may have quality issues)
- **0.25-0.35**: Acceptable match (default threshold: 0.25)
- **> 0.35**: Good semantic match

Adjust `--positive-threshold` based on your quality requirements:
```bash
# Strict validation (higher quality bar)
python3 generate.py ... --validate --positive-threshold 0.30

# Lenient validation (accept more variations)
python3 generate.py ... --validate --positive-threshold 0.20
```

### Example: Generating a Single Car

**Problem**: Often get duplicate/cloned cars

**Solution**: Use validation with strong emphasis on "single"

```bash
python3 generate.py --workflow workflows/flux-dev.json \
    --prompt "(Porsche 911:2.0) (single car:1.5) (one car only:1.5), driving on mountain road, photorealistic" \
    --negative-prompt "multiple cars, two cars, duplicate, cloned, ghosting, mirrored, extra car" \
    --output /tmp/porsche.png \
    --validate --auto-retry --retry-limit 3 \
    --positive-threshold 0.28
```

### Dependencies

Validation requires:
```bash
pip install transformers
```

(torch and PIL are already in requirements.txt)

### Troubleshooting

**"CLIP dependencies not available"**
- Install: `pip install transformers`
- May also need to update torch: `pip install --upgrade torch`

**Validation always fails**
- Lower threshold: `--positive-threshold 0.20`
- Check prompt matches expected output
- Try without negative prompt first

**Too many retries**
- Reduce retry limit: `--retry-limit 2`
- Review prompt for contradictions
- Consider if validation threshold is too strict

---

## Error Handling

### Best Practices

1. **Always check server availability first**
   ```python
   if not check_server_availability():
       print("[ERROR] Server is down")
       sys.exit(2)
   ```

2. **Use dry-run mode for new workflows**
   ```bash
   python3 generate.py --workflow new_workflow.json --dry-run
   ```

3. **Handle missing models gracefully**
   ```python
   models = get_available_models()
   is_valid, missing, suggestions = validate_workflow_models(workflow, models)
   if not is_valid:
       print(f"Missing: {missing}")
       print(f"Suggestions: {suggestions}")
   ```

4. **Use automatic retry for transient failures**
   - Built-in retry logic handles network errors automatically
   - 3 retries with exponential backoff (2s, 4s, 8s)

5. **Clean up on cancellation**
   ```python
   import signal
   
   def signal_handler(signum, frame):
       cancel_prompt(current_prompt_id)
       cleanup_partial_output(output_path)
       sys.exit(0)
   
   signal.signal(signal.SIGINT, signal_handler)
   ```

### Common Error Scenarios

| Error | Cause | Solution |
|-------|-------|----------|
| "Cannot connect to server" | ComfyUI down | Start via `scripts/start_comfyui.py` |
| "Workflow validation failed" | Missing models | Check MODEL_REGISTRY.md, use suggested fallbacks |
| "Invalid JSON in workflow" | Malformed workflow | Re-export from ComfyUI |
| "Input image not found" | Wrong path | Verify file exists, use absolute paths |
| "Failed to upload to MinIO" | MinIO down/misconfigured | Check MinIO at 192.168.1.215:9000 |

### Exit Codes

- **0**: Success
- **1**: Generation or runtime failure
- **2**: Configuration error (server down, missing models, etc.)

Use exit codes in scripts:
```bash
python3 generate.py ... || echo "Generation failed with code $?"
```

---

## End-to-End Examples

### Example 1: Simple Image with Validation

**Goal**: Generate a high-quality image with automatic retry

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a serene mountain lake at sunset, reflection in water, photorealistic, 8k" \
    --negative-prompt "blurry, low quality, distorted, watermark" \
    --output /tmp/lake.png \
    --validate \
    --auto-retry \
    --retry-limit 3 \
    --positive-threshold 0.28
```

**Expected output:**
- Initial generation attempt
- CLIP validation score
- Automatic retry if score < 0.28
- Final validated image uploaded to MinIO

**Use when:**
- Quality is critical
- Subject is simple and well-defined
- You want consistency

---

### Example 2: Image Transformation

**Goal**: Transform a photo to artistic style

```bash
python3 generate.py \
    --workflow workflows/sd15-img2img.json \
    --input-image http://192.168.1.215:9000/comfy-gen/original_photo.png \
    --resize 512x512 \
    --crop cover \
    --denoise 0.7 \
    --prompt "watercolor painting, soft colors, artistic, impressionist style" \
    --negative-prompt "photograph, realistic, sharp, detailed" \
    --output /tmp/artistic.png
```

**Expected output:**
- Downloads original_photo.png
- Resizes to 512x512 with cover crop
- Transforms to watercolor style
- Denoise 0.7 allows creative freedom while maintaining composition

**Use when:**
- You have an existing image to transform
- You want to maintain the basic structure but change style
- Artistic effects needed

---

### Example 3: Video Generation

**Goal**: Create a 10-second video from text

```bash
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "a drone shot flying over a coastal highway, waves crashing, sunset lighting, cinematic" \
    --output /tmp/coastal_video.mp4
```

**Expected output:**
- 848x480 resolution video
- 81 frames at 8 fps (~10 seconds)
- 4-step generation (fast with acceleration LoRA)
- Cinematic drone movement

**Use when:**
- Video/animation is requested
- No input image available
- Scene involves motion or camera movement

**Note**: Generation time is 2-5 minutes depending on GPU load

---

### Example 4: Dry-Run Validation

**Goal**: Validate a new workflow before running

```bash
python3 generate.py \
    --workflow workflows/custom_workflow.json \
    --dry-run
```

**Expected output:**
- Server availability check: PASS
- Workflow load: PASS  
- Model validation: PASS or FAIL with suggestions
- No generation performed

**Use when:**
- Testing a new workflow
- Verifying model availability
- Batch validation of multiple workflows

**Example batch validation:**
```bash
for workflow in workflows/*.json; do
    echo "Validating $workflow..."
    python3 generate.py --workflow "$workflow" --dry-run || echo "FAILED: $workflow"
done
```

---

### Example 5: Programmatic Generation

**Goal**: Generate multiple images in a Python script

```python
#!/usr/bin/env python3
from generate import *

prompts = [
    "a sunset over mountains",
    "a sports car on a highway",
    "a serene lake with reflections",
]

workflow = load_workflow("workflows/flux-dev.json")

for i, prompt in enumerate(prompts):
    print(f"Generating {i+1}/{len(prompts)}: {prompt}")
    
    workflow = modify_prompt(workflow, prompt, "blurry, low quality")
    prompt_id = queue_workflow(workflow)
    
    if prompt_id:
        status = wait_for_completion(prompt_id)
        output_path = f"/tmp/batch_{i}.png"
        
        if download_output(status, output_path):
            url = upload_to_minio(output_path, f"batch_{i}.png")
            print(f"Done: {url}")
```

**Use when:**
- Batch generation needed
- Programmatic control required
- Integration with other systems

---

## Summary Checklist

Before generating, ensure:

- [ ] ComfyUI server is running (check with `check_server_availability()`)
- [ ] Correct workflow selected based on decision tree
- [ ] Models exist (use `--dry-run` or `validate_workflow_models()`)
- [ ] Prompts are well-structured (see Prompt Engineering section)
- [ ] Input images are prepared if needed (resize, crop)
- [ ] Validation enabled if quality is critical (`--validate`)
- [ ] Output path is writable
- [ ] MinIO is accessible for upload

After generation:

- [ ] Check exit code (0 = success, 1 = failure, 2 = config error)
- [ ] Verify MinIO URL is accessible
- [ ] Review validation scores if enabled
- [ ] Clean up temporary files

For more details, see:
- [ERROR_HANDLING.md](ERROR_HANDLING.md) - Comprehensive error handling guide
- [API_REFERENCE.md](API_REFERENCE.md) - Function-level documentation
- [MODEL_REGISTRY.md](MODEL_REGISTRY.md) - Model inventory and specs
