# Agent Generation Guide

This guide is for AI agents and low-level workers to understand how to generate images and videos using ComfyGen.

## Quick Reference

### Simple Image Generation (SD 1.5)

```bash
# From magneto or any machine with SSH access
python3 generate.py \
    --workflow workflows/sd15-basic.json \
    --prompt "a red sports car on a mountain road, cinematic lighting" \
    --output /tmp/car.png
```

The image will be:
1. Generated by ComfyUI on moira
2. Saved locally to `/tmp/car.png`
3. Uploaded to MinIO at `http://192.168.1.215:9000/comfy-gen/<timestamp>_car.png`

### Image Generation with Validation

Use `--validate` to check if the generated image matches the prompt using CLIP:

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a single red Porsche 911 on a mountain road" \
    --negative-prompt "multiple cars, duplicate, cloned" \
    --output /tmp/car.png \
    --validate \
    --validation-threshold 0.25
```

### Auto-Retry on Validation Failure

Use `--auto-retry` to automatically regenerate images that fail validation:

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a single red Porsche 911 on a mountain road" \
    --negative-prompt "multiple cars, duplicate, blurry" \
    --output /tmp/car.png \
    --validate --auto-retry --retry-limit 3
```

When validation fails, the system will:
1. Strengthen positive prompt emphasis (e.g., add "single subject", increase weight)
2. Add negative terms: "duplicate", "cloned", "multiple", "ghosting", "mirrored"
3. Retry generation (up to retry-limit attempts)
4. Report validation scores and diagnostics

### Viewing Generated Images

Images are viewable directly in browser (no download):
```
http://192.168.1.215:9000/comfy-gen/<filename>.png
```

List all images:
```bash
curl -s http://192.168.1.215:9000/comfy-gen/ | grep -oP '(?<=<Key>)[^<]+'
```

---

## Understanding Workflows

A workflow is a JSON file that defines:
1. **What model to use** (checkpoint)
2. **What prompt to encode** (positive/negative)
3. **How to sample** (steps, CFG, sampler)
4. **What LoRAs to apply** (optional)

### Basic SD 1.5 Workflow Structure

```json
{
  "3": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "v1-5-pruned-emaonly-fp16.safetensors"
    }
  },
  "6": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "YOUR PROMPT HERE",
      "clip": ["3", 1]
    }
  },
  "7": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "bad quality, blurry",
      "clip": ["3", 1]
    }
  }
}
```

---

## Using LoRAs

LoRAs modify the model to achieve specific styles or effects. They are applied between the checkpoint loader and the sampler.

### LoraLoader Node Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | MODEL | - | Input from CheckpointLoader |
| `clip` | CLIP | - | Input from CheckpointLoader |
| `lora_name` | string | - | Filename of the LoRA |
| `strength_model` | float | 1.0 | How strongly to apply LoRA to model (-100 to 100) |
| `strength_clip` | float | 1.0 | How strongly to apply LoRA to CLIP (-100 to 100) |

### Example: Adding a LoRA to Workflow

```json
{
  "3": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "v1-5-pruned-emaonly-fp16.safetensors"
    }
  },
  "10": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["3", 0],
      "clip": ["3", 1],
      "lora_name": "example_lora.safetensors",
      "strength_model": 0.8,
      "strength_clip": 0.8
    }
  },
  "6": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "YOUR PROMPT",
      "clip": ["10", 1]
    }
  }
}
```

### Chaining Multiple LoRAs

```json
{
  "10": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["3", 0],
      "clip": ["3", 1],
      "lora_name": "first_lora.safetensors",
      "strength_model": 0.7,
      "strength_clip": 0.7
    }
  },
  "11": {
    "class_type": "LoraLoader",
    "inputs": {
      "model": ["10", 0],
      "clip": ["10", 1],
      "lora_name": "second_lora.safetensors",
      "strength_model": 0.5,
      "strength_clip": 0.5
    }
  }
}
```

### LoRA Strength Guidelines

| Strength | Effect |
|----------|--------|
| 0.3-0.5 | Subtle influence |
| 0.6-0.8 | Moderate effect |
| 0.9-1.0 | Strong effect |
| 1.0+ | May cause artifacts |
| Negative | Inverse effect |

---

## Available Models

### Base Models (Checkpoints)

| Model | Filename | Best For |
|-------|----------|----------|
| SD 1.5 | `v1-5-pruned-emaonly-fp16.safetensors` | General images, fast |

### Video Models

| Model | Filename | Type |
|-------|----------|------|
| Wan 2.2 T2V High | `wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors` | Text-to-video |
| Wan 2.2 T2V Low | `wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors` | Text-to-video |

### Acceleration LoRAs (Reduce Steps)

These LoRAs allow generating in fewer steps:

| LoRA | Compatible With | Steps |
|------|----------------|-------|
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors` | Wan T2V High | 4 |
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors` | Wan T2V Low | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors` | Wan I2V High | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors` | Wan I2V Low | 4 |

---

## Decision Tree for Agents

### When to Use SD 1.5
- Simple image requests
- Fast generation needed
- Generic subjects (landscapes, objects, architecture)

### When to Use Wan 2.2
- Video generation requested
- Animation or motion needed
- Text-to-video: Use `workflows/wan22-t2v.json`
- Image-to-video: Use `workflows/wan22-i2v.json`
- Longer generation time acceptable (4 steps with acceleration LoRA)

### Choosing LoRA Strength
1. For acceleration LoRAs (4-step): Use `strength_model: 1.0, strength_clip: 1.0`
2. For style LoRAs: Start with `strength_model: 0.7, strength_clip: 0.7`
3. If effect too subtle, increase to 0.9
4. If artifacts appear, decrease to 0.5
5. For physics/motion LoRAs, use 0.6-0.8

---

## Error Handling

### ComfyUI Not Responding
```bash
# Check status
curl -s http://192.168.1.215:8188/system_stats

# If no response, start ComfyUI
ssh moira "C:\\Users\\jrjen\\comfy\\.venv\\Scripts\\python.exe C:\\Users\\jrjen\\comfy-gen\\scripts\\start_comfyui.py"
```

### Model Not Found
1. Check exact filename in MODEL_REGISTRY.md
2. Verify model exists: `ssh moira "dir C:\\Users\\jrjen\\comfy\\models\\checkpoints"`
3. Model names are case-sensitive

### LoRA Not Found
1. List available LoRAs via API:
   ```bash
   curl -s http://192.168.1.215:8188/object_info | python3 -c "
   import json,sys
   data=json.load(sys.stdin)
   for l in data['LoraLoader']['input']['required']['lora_name'][0]:
       print(l)
   "
   ```

---

## Prompt Engineering Tips

### For SD 1.5

**Good prompt structure:**
```
[subject], [style], [lighting], [quality modifiers]
```

**Example:**
```
a red Porsche 911 on a mountain road, cinematic photography, golden hour lighting, highly detailed, 8k
```

**Negative prompt (always include):**
```
bad quality, blurry, low resolution, watermark, text
```

### For Video (Wan 2.2)

**Good prompt structure:**
```
[action/motion], [subject], [setting], [camera movement]
```

**Example:**
```
a car driving along a coastal highway, waves crashing, drone shot following
```

---

## Workflow Templates

See `workflows/` directory for ready-to-use templates:

| Template | Use Case | Notes |
|----------|----------|-------|
| `flux-dev.json` | Simple SD 1.5 images | Fast, general purpose |
| `wan22-t2v.json` | Text-to-video | Uses Wan 2.2 with 4-step LoRA |
| `wan22-i2v.json` | Image-to-video | Animates existing images |

### Using Wan 2.2 Workflows

**Text-to-Video Example:**
```bash
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "a person walking through a park on a sunny day" \
    --output /tmp/video.mp4
```

**Image-to-Video Example:**
```bash
# Note: Requires modifying the workflow to specify input image
python3 generate.py \
    --workflow workflows/wan22-i2v.json \
    --prompt "the person starts walking forward" \
    --output /tmp/animated.mp4
```

**Wan 2.2 Video Specifications:**
- **Resolution:** 848x480 pixels
- **Frame Count:** 81 frames
- **Frame Rate:** 8 fps
- **Duration:** ~10 seconds
- **Steps:** 4 (with acceleration LoRA)
- **CFG:** 1.0 (recommended with 4-step LoRA)

To create a new workflow:
1. Export from ComfyUI GUI
2. Save to `workflows/` with descriptive name
3. Document in this file

---

## Image Validation and Quality Control

### Validation Overview

ComfyGen includes CLIP-based validation to verify generated images match prompts. This is useful for:
- Detecting duplicate/cloned subjects (e.g., multiple cars instead of one)
- Verifying semantic similarity between image and prompt
- Automatically retrying with adjusted prompts on failure

### Basic Validation

Add `--validate` to check the image after generation:

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a single red Porsche 911" \
    --output /tmp/car.png \
    --validate
```

**Output:**
```
[INFO] Validating image...
[OK] Image matches prompt (score: 0.342 >= 0.25), low negative score (0.089)
  Positive score: 0.342
  Negative score: 0.089
[OK] Validation passed!
```

### Auto-Retry on Failure

Use `--auto-retry` to automatically regenerate failed images:

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a single red Porsche 911 on a mountain road" \
    --negative-prompt "multiple cars, duplicate" \
    --output /tmp/car.png \
    --validate --auto-retry --retry-limit 3
```

**What happens on retry:**
1. **Prompt strengthening**: Adds "single subject", "one" to positive prompt
2. **Weight increase**: Wraps prompt with `(...:1.3)` → `(...:1.6)` → `(...:1.9)`
3. **Negative expansion**: Adds "duplicate", "cloned", "multiple", "ghosting", "mirrored"
4. **Regeneration**: Runs workflow with adjusted prompts

### Validation Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--validate` | flag | false | Enable validation |
| `--auto-retry` | flag | false | Retry on failure |
| `--retry-limit` | int | 3 | Max retry attempts |
| `--validation-threshold` | float | 0.25 | CLIP score threshold (0-1) |

### CLIP Score Interpretation

| Score Range | Meaning |
|-------------|---------|
| 0.35+ | Strong match |
| 0.25-0.35 | Good match (default threshold) |
| 0.15-0.25 | Weak match |
| < 0.15 | Poor match |

### When to Use Validation

**Use validation when:**
- Generating subjects where count matters (e.g., "one car", "single person")
- Quality is critical and retries are acceptable
- You want automated quality control

**Skip validation when:**
- Speed is critical (adds ~5-10s per validation)
- Generating abstract/artistic content (CLIP may not assess well)
- Running batch jobs where manual review is planned

### Example: High-Quality Car Generation

```bash
# Generate a single car with strict validation
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "(single Porsche 911:2.0), one car only, isolated subject, mountain road, cinematic" \
    --negative-prompt "multiple cars, duplicate, cloned, ghosting, mirrored, two cars, several cars" \
    --output /tmp/porsche.png \
    --validate --auto-retry --retry-limit 5 \
    --validation-threshold 0.30
```

This will:
- Emphasize "single" and "one car" with weight 2.0
- Retry up to 5 times on failure
- Require CLIP score >= 0.30 (higher than default)
- Progressively strengthen prompts on each retry

