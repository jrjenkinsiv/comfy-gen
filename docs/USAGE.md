# ComfyGen Usage Guide

Complete guide for using ComfyGen via CLI and MCP server for AI-driven image and video generation.

## Table of Contents

- [Quick Start](#quick-start)
- [CLI Usage](#cli-usage)
  - [Basic Image Generation](#basic-image-generation)
  - [Image-to-Image](#image-to-image)
  - [Video Generation](#video-to-image)
  - [Parameters](#parameters)
  - [Presets](#presets)
  - [Validation](#validation)
  - [Quality Scoring](#quality-scoring)
- [MCP Server](#mcp-server)
  - [Setup](#mcp-setup)
  - [Available Tools](#available-tools)
  - [MCP Examples](#mcp-examples)
- [Workflows](#workflows)
- [Prompt Engineering](#prompt-engineering)
- [Error Handling](#error-handling)
- [Gallery Browser](#gallery-browser)

---

## Quick Start

### Simple Image Generation

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a sunset over mountains, cinematic lighting" \
    --output /tmp/sunset.png
```

The image will be:
1. Generated by ComfyUI on moira
2. Saved locally to `/tmp/sunset.png`
3. Uploaded to MinIO at `http://192.168.1.215:9000/comfy-gen/<timestamp>_sunset.png`

### Viewing Images

Images are viewable directly in browser:
```
http://192.168.1.215:9000/comfy-gen/<filename>.png
```

List all images:
```bash
curl -s http://192.168.1.215:9000/comfy-gen/ | grep -oP '(?<=<Key>)[^<]+'
```

---

## CLI Usage

### Basic Image Generation

```bash
# SD 1.5 generation
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "detailed portrait of a warrior, highly detailed" \
    --negative-prompt "blurry, low quality, watermark" \
    --output /tmp/warrior.png

# With custom parameters
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "landscape with mountains" \
    --steps 50 \
    --cfg 7.5 \
    --width 768 \
    --height 512 \
    --seed 12345 \
    --output /tmp/landscape.png

# Using a preset
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "detailed fantasy scene" \
    --preset high-quality \
    --output /tmp/scene.png
```

### Image-to-Image

Transform existing images with prompts:

```bash
# Basic transformation
python3 generate.py \
    --workflow workflows/sd15-img2img.json \
    --input-image /path/to/source.png \
    --prompt "oil painting style" \
    --denoise 0.7 \
    --output /tmp/artistic.png

# From URL with preprocessing
python3 generate.py \
    --workflow workflows/sd15-img2img.json \
    --input-image "http://192.168.1.215:9000/comfy-gen/previous.png" \
    --resize 512x512 \
    --crop cover \
    --denoise 0.5 \
    --prompt "watercolor painting, soft colors" \
    --output /tmp/watercolor.png
```

**Denoise Guide:**
- **0.3**: Very subtle changes
- **0.5**: Moderate changes, keeps structure
- **0.7**: Significant transformation
- **0.9**: Heavy transformation

### Video Generation

```bash
# Text-to-video
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "drone shot flying over coastal highway, waves crashing, cinematic" \
    --output /tmp/coastal.mp4

# Image-to-video (animate existing image)
python3 generate.py \
    --workflow workflows/wan22-i2v.json \
    --input-image /path/to/photo.png \
    --prompt "camera slowly zooms in, subtle movement" \
    --output /tmp/animated.mp4

# With LoRAs
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "dancer performing, dynamic movement" \
    --lora "BoobPhysics_WAN_v6.safetensors:0.7" \
    --lora "BounceHighWan2_2.safetensors:0.6" \
    --output /tmp/dance.mp4
```

### Parameters

| Parameter | Range | Default | Effect |
|-----------|-------|---------|--------|
| `--steps` | 1-150 | 20 | Sampling steps (more = finer detail) |
| `--cfg` | 1.0-20.0 | 7.0 | Prompt adherence (higher = stricter) |
| `--seed` | -1 or int | random | Random seed (**avoid for exploration**) |
| `--width` | 64-2048 | 512 | Output width (divisible by 8) |

> ⚠️ **WARNING**: Avoid specifying `--seed` for batch generation or exploration.
> Fixed seeds produce deterministic outputs, preventing variety. Only use seeds
> when you need exact reproducibility (e.g., demonstrating a specific result).
| `--height` | 64-2048 | 512 | Output height (divisible by 8) |
| `--sampler` | string | varies | Sampler algorithm |
| `--scheduler` | string | normal | Noise scheduler |
| `--denoise` | 0.0-1.0 | 1.0 | Denoising strength (img2img) |

**Samplers:**
- `euler` - Fast, simple, good for drafts
- `euler_ancestral` - Adds randomness
- `dpmpp_2m` - Fast with good quality
- `dpmpp_2m_sde` - Slower but higher quality

**Schedulers:**
- `normal` - Standard linear schedule
- `karras` - Better detail preservation
- `exponential` - Alternative distribution

### Presets

Use `--preset` for predefined parameter combinations:

| Preset | Steps | CFG | Sampler | Use Case |
|--------|-------|-----|---------|----------|
| `draft` | 10 | 5.0 | euler | Quick previews |
| `balanced` | 20 | 7.0 | euler_ancestral | Default quality/speed |
| `high-quality` | 50 | 7.5 | dpmpp_2m_sde | Final outputs |
| `fast` | 15 | 7.0 | dpmpp_2m | Good speed/quality |
| `ultra` | 100 | 8.0 | dpmpp_2m_sde | Maximum quality |

Example:
```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "detailed scene" \
    --preset high-quality \
    --output scene.png
```

Override specific parameters:
```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "robot" \
    --preset balanced \
    --seed 42 \
    --width 768 \
    --output robot.png
```

### Validation

ComfyGen includes CLIP-based validation to detect quality issues and auto-retry with adjusted prompts.

```bash
# Just validate (no retry)
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a red Porsche 911" \
    --output /tmp/car.png \
    --validate

# Validate with auto-retry
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "(Porsche 911:2.0) single car, one car only" \
    --negative-prompt "multiple cars, duplicate, cloned" \
    --output /tmp/car.png \
    --validate --auto-retry --retry-limit 3

# Validate with person count detection (YOLO)
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "solo woman standing in a field, detailed portrait" \
    --output /tmp/portrait.png \
    --validate --validate-person-count

# Person count with auto-retry
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "two women talking, casual conversation" \
    --output /tmp/conversation.png \
    --validate --validate-person-count --auto-retry
```

**Validation Options:**

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| `--validate` | Boolean | False | Run validation after generation |
| `--validate-person-count` | Boolean | False | Validate person count using YOLO (requires `--validate`) |
| `--auto-retry` | Boolean | False | Retry if validation fails |
| `--retry-limit` | Integer | 3 | Maximum retry attempts |
| `--positive-threshold` | Float | 0.25 | Minimum CLIP score (0-1) |

**CLIP Score Interpretation:**
- **< 0.20**: Poor semantic match
- **0.20-0.25**: Marginal match
- **0.25-0.35**: Acceptable (default threshold)
- **> 0.35**: Good semantic match

**Person Count Validation:**

Person count validation uses YOLOv8 to detect and count persons in generated images. This helps catch issues where CLIP semantic similarity alone cannot distinguish between "solo woman" and "two women".

**Installation:**
```bash
pip install ultralytics
```

**Supported Keywords for Person Count:**
- **"solo", "single"** → Expects 1 person
- **"one person", "one woman", "one man"** → Expects 1 person
- **"two people", "two women", "two men"** → Expects 2 persons
- **"three people", "four women", etc.** → Expects N persons (supports up to "ten")
- **"group of five"** → Expects 5 persons
- **Number + person/people/woman/man** (e.g., "5 people") → Expects specified count

**How It Works:**
1. Extracts expected person count from prompt using keyword patterns
2. Runs YOLOv8 detection on generated image (runs on CPU)
3. Compares detected count with expected count
4. Fails validation if counts don't match

**Use Cases:**
- Portrait generation where exact subject count matters
- Avoiding duplicate/merged subjects (common with certain models)
- Ensuring "solo" prompts don't generate multiple subjects
- Validating group shots have correct number of people

**Example Output:**
```
[INFO] Running validation...
[INFO] Detected persons: 2
[INFO] Expected persons: 1
[INFO] Validation result: Person count mismatch: expected 1, detected 2
[WARN] Image failed validation: Person count mismatch: expected 1, detected 2
```

---

### Quality Scoring

ComfyGen includes multi-dimensional quality assessment using pyiqa for comprehensive image quality analysis.

#### Installation

```bash
pip install pyiqa
```

#### Basic Usage

```bash
# Generate with quality assessment
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a majestic lion in golden hour lighting" \
    --output /tmp/lion.png \
    --quality-score

# Output:
# [OK] Quality Grade: B (Score: 7.8/10)
# [INFO] Technical: 7.2/10, Aesthetic: 8.1/10, Detail: 7.8/10
# [INFO] Prompt Adherence: 8.5/10 (CLIP)
```

#### Standalone Quality Assessment

Score existing images:

```bash
# Score an image without prompt
python3 -m comfy_gen.quality /path/to/image.png

# Score with prompt for adherence checking
python3 -m comfy_gen.quality /path/to/image.png "a majestic lion"

# Output:
# ============================================================
# Quality Assessment Results
# ============================================================
# 
# Overall Grade: B (Composite Score: 7.8/10)
# 
# Dimension Breakdown:
#   Technical Quality:    7.2/10 (BRISQUE)
#                         6.8/10 (NIQE)
#   Aesthetic Quality:    8.1/10
#   Detail Quality:       7.8/10
#   Prompt Adherence:     8.5/10 (CLIP)
# 
# Grade Scale:
#   A (8.0-10.0): Production ready
#   B (6.5-7.9):  Good, minor issues
#   C (5.0-6.4):  Acceptable
#   D (3.0-4.9):  Poor
#   F (0.0-2.9):  Failed
# ============================================================
```

#### Quality Dimensions

| Dimension | Metrics | What It Measures |
|-----------|---------|------------------|
| **Technical** | BRISQUE, NIQE | Artifacts, noise, blur, jaggedness |
| **Aesthetic** | LAION Aesthetic | Visual appeal, composition |
| **Prompt Adherence** | CLIP | Does image match text description |
| **Detail** | TOPIQ | Fine detail preservation, textures |

#### Composite Score Formula

```python
quality_score = (
    0.30 * technical_score +    # Artifacts, sharpness
    0.25 * aesthetic_score +    # Visual appeal
    0.25 * prompt_adherence +   # Matches request
    0.20 * detail_score         # Fine details
)
```

#### Grade Thresholds

| Grade | Score Range | Description |
|-------|-------------|-------------|
| **A** | 8.0 - 10.0 | Production ready |
| **B** | 6.5 - 7.9 | Good, minor issues |
| **C** | 5.0 - 6.4 | Acceptable |
| **D** | 3.0 - 4.9 | Poor |
| **F** | 0.0 - 2.9 | Failed |

#### Metadata Integration

Quality scores are automatically stored in metadata JSON:

```json
{
  "quality": {
    "composite_score": 7.8,
    "grade": "B",
    "technical": {
      "brisque": 7.2,
      "niqe": 6.8
    },
    "aesthetic": 8.1,
    "detail": 7.8,
    "prompt_adherence": {
      "clip": 8.5
    }
  }
}
```

#### Gallery Display

The gallery server automatically displays quality grades with color-coded badges:
- **A** (Green) - Production ready
- **B** (Light green) - Good quality
- **C** (Yellow) - Acceptable
- **D** (Orange) - Poor
- **F** (Red) - Failed

Visit `http://localhost:8080` after running `python3 scripts/gallery_server.py`

---

## MCP Server

The MCP (Model Context Protocol) server provides 25 AI-ready tools for image and video generation, allowing AI assistants like Claude to control ComfyGen.

### MCP Setup

Add to your MCP client configuration (VS Code, Claude Desktop, etc.):

```json
{
  "mcpServers": {
    "comfy-gen": {
      "command": "python3",
      "args": ["/path/to/comfy-gen/mcp_server.py"],
      "env": {
        "COMFYUI_HOST": "http://192.168.1.215:8188",
        "MINIO_ENDPOINT": "192.168.1.215:9000",
        "MINIO_BUCKET": "comfy-gen",
        "CIVITAI_API_KEY": "${CIVITAI_API_KEY}"
      }
    }
  }
}
```

**Environment Variables:**
- `COMFYUI_HOST` - ComfyUI server URL (default: http://192.168.1.215:8188)
- `MINIO_ENDPOINT` - MinIO endpoint (default: 192.168.1.215:9000)
- `MINIO_BUCKET` - MinIO bucket name (default: comfy-gen)
- `CIVITAI_API_KEY` - Optional CivitAI API key for model downloads

### Available Tools

The MCP server provides tools across 7 categories:

#### 1. Service Management (4 tools)

- `start_comfyui_service()` - Start ComfyUI server
- `stop_comfyui_service()` - Stop ComfyUI server
- `restart_comfyui_service()` - Restart ComfyUI server
- `check_comfyui_service_status()` - Check server status

#### 2. Image Generation (2 tools)

**`generate_image(prompt, ...)`**

Generate image from text prompt with optional progress tracking and local file output.

Args:
- `prompt` (str): What to generate
- `negative_prompt` (str): What to avoid (default: "blurry, low quality, watermark")
- `model` (str): Model to use - sd15, flux, sdxl (default: sd15)
- `width` (int): Image width (default: 512)
- `height` (int): Image height (default: 512)
- `steps` (int): Sampling steps (default: 20)
- `cfg` (float): CFG scale (default: 7.0)
- `sampler` (str): Sampler algorithm (default: euler)
- `scheduler` (str): Scheduler type (default: normal)
- `seed` (int): Random seed, -1 for random (default: -1)
- `output_path` (str): Optional local file path to save image
- `json_progress` (bool): Enable structured progress updates (default: False)

Returns: `{status, url, local_path, prompt_id, metadata, progress_updates}`

**Progress Tracking Example:**
```python
result = await generate_image(
    prompt="a sunset over mountains, cinematic lighting",
    output_path="/tmp/sunset.png",
    json_progress=True
)

# Access progress updates
for update in result["progress_updates"]:
    if update["type"] == "progress":
        print(f"Step {update['step']}/{update['max_steps']} ({update['percent']}%)")

# Image saved to both:
print(f"MinIO URL: {result['url']}")
print(f"Local file: {result['local_path']}")
```

**`img2img(input_image, prompt, ...)`**

Transform existing image with prompt guidance.

Args:
- `input_image` (str): URL or path to input image
- `prompt` (str): Transformation prompt
- `negative_prompt` (str): What to avoid
- `denoise` (float): Strength 0.0-1.0 (default: 0.7)
- Additional parameters same as `generate_image`

Returns: `{status, url, prompt_id, metadata}`

#### 3. Video Generation (2 tools)

**`generate_video(prompt, ...)`**

Generate video from text using Wan 2.2 T2V.

Args:
- `prompt` (str): Video description
- `negative_prompt` (str): What to avoid
- `width` (int): Video width (default: 832)
- `height` (int): Video height (default: 480)
- `frames` (int): Number of frames (default: 81)
- `fps` (int): Frames per second (default: 16)
- `steps` (int): Sampling steps (default: 30)
- `cfg` (float): CFG scale (default: 6.0)
- `seed` (int): Random seed (default: -1)

Returns: `{status, url, prompt_id, metadata}`

**`image_to_video(input_image, prompt, ...)`**

Animate image to video using Wan 2.2 I2V.

Args:
- `input_image` (str): URL or path to input image
- `prompt` (str): Motion description
- `motion_strength` (float): Movement amount 0.0-1.0+ (default: 1.0)
- Additional parameters same as `generate_video`

Returns: `{status, url, prompt_id, metadata}`

#### 4. Model Management (6 tools)

- `list_models()` - List installed checkpoint models
- `list_loras()` - List installed LoRAs with compatibility info
- `get_model_info(model_name)` - Get detailed metadata about a model
- `suggest_model(task, style, subject)` - Recommend best model for a task
- `suggest_loras(prompt, model, max_suggestions)` - Recommend LoRAs based on prompt
- `search_civitai(query, model_type, ...)` - Search CivitAI for models and LoRAs

#### 5. Gallery & History (4 tools)

- `list_images(limit, prefix, sort)` - Browse generated images
- `get_image_info(image_name)` - Get generation parameters for an image
- `delete_image(image_name)` - Remove image from storage
- `get_history(limit)` - Get recent generations with full parameters

#### 6. Prompt Engineering (3 tools)

- `build_prompt(subject, style, setting)` - Construct a well-formed prompt
- `suggest_negative(model_type)` - Get recommended negative prompt
- `analyze_prompt(prompt)` - Analyze prompt and suggest improvements

#### 7. Progress & Control (5 tools)

- `get_progress(prompt_id)` - Get current generation progress
- `cancel(prompt_id)` - Cancel current or specific generation
- `get_queue()` - View queued jobs
- `get_system_status()` - Get GPU/VRAM/server health info
- `validate_workflow(model, prompt, width, height)` - Validate workflow without generating (dry run)

**Progress Tracking:**
```python
# Start generation
result = await generate_image(prompt="test", json_progress=False)

# Poll for progress
progress = await get_progress(result["prompt_id"])
# Returns: {"status": "running", "position": "current", ...}
```

**Dry Run Validation:**
```python
# Validate before generating
validation = await validate_workflow(
    model="sd15",
    prompt="test",
    width=512,
    height=512
)

if validation["is_valid"]:
    # Proceed with actual generation
    result = await generate_image(...)
else:
    print(f"Validation errors: {validation['errors']}")
    print(f"Missing models: {validation['missing_models']}")
```

### MCP Examples

**Simple Image Generation:**
```python
result = await generate_image(
    prompt="a sunset over mountains, highly detailed, 8k",
    width=768,
    height=512,
    steps=25
)
# Returns: {"status": "success", "url": "http://...", ...}
```

**Intelligent Model Selection:**
```python
# Get model recommendation
model = await suggest_model(task="portrait", style="realistic")

# Get LoRA suggestions
loras = await suggest_loras(
    prompt="woman portrait with detailed face",
    model=model["recommended"]
)

# Generate with suggestions
result = await generate_image(
    prompt="woman portrait, professional lighting",
    model=model["recommended"]
)
```

**Animated Portrait:**
```python
# Generate base image
image = await generate_image(
    prompt="woman portrait, professional photo",
    width=832,
    height=480
)

# Animate to video
video = await image_to_video(
    input_image=image["url"],
    prompt="subtle head turn, slight smile",
    steps=30
)
```

**Gallery Management:**
```python
# List recent images
images = await list_images(limit=10, sort="newest")

# Get generation details
info = await get_image_info(images["images"][0]["name"])
print(f"Prompt: {info['generation_params']['positive_prompt']}")

# Delete old images
await delete_image("old_image.png")
```

---

## Workflows

ComfyGen includes several pre-built workflows for different tasks:

| Workflow | Type | Input | Output | Time | Use Case |
|----------|------|-------|--------|------|----------|
| `flux-dev.json` | T2I | None | 512x512 PNG | 10-15s | Quick image generation |
| `sd15-img2img.json` | I2I | Image | Variable PNG | 10-20s | Image transformation |
| `wan22-t2v.json` | T2V | None | 848x480 MP4 | 2-5min | Video from text |
| `wan22-i2v.json` | I2V | Image | 848x480 MP4 | 2-5min | Animate images |

**Decision Tree:**

```
Need video/animation?
├─ YES → Input image available?
│         ├─ YES → wan22-i2v.json (Image-to-Video)
│         └─ NO  → wan22-t2v.json (Text-to-Video)
│
└─ NO  → Transform existing image?
          ├─ YES → sd15-img2img.json
          └─ NO  → flux-dev.json (SD 1.5)
```

---

## Prompt Engineering

### Prompt Catalog

A structured collection of prompt patterns is available in `prompt_catalog.yaml`:

- **Quality Boosters** - Phrases to enhance image quality
- **Negative Presets** - Curated negative prompts by use case (minimal, standard, comprehensive)
- **Style Modifiers** - Photography, pixel art, vector, illustration styles
- **Templates** - Ready-to-use prompts for common scenarios (automotive, game icons, portraits)
- **Model-Specific Adjustments** - CFG and step recommendations by model

Use the catalog to build consistent, high-quality prompts.

### CRITICAL: Use Detailed, Verbose Prompts

Models can handle paragraph-length prompts with extensive detail. More specific descriptions produce better results.

**Basic Structure (Minimum):**
```
[subject], [style], [lighting], [quality modifiers]
```

**Example (basic):**
```
a red Porsche 911 on a mountain road, cinematic photography, golden hour lighting, highly detailed, 8k
```

**Recommended: Detailed Paragraph-Style Prompts:**

```
A sleek red Porsche 911 GT3 sports car driving along a winding mountain road carved into 
rocky cliffs, photographed during golden hour with warm amber sunlight streaming through 
gaps in the mountains, casting long dramatic shadows across the asphalt. The car is 
captured in motion from a three-quarter front angle, showing the aggressive front fascia 
and aerodynamic bodywork in sharp detail. The background features towering pine trees and 
distant snow-capped peaks bathed in soft orange-pink evening light. Professional 
automotive photography style reminiscent of Top Gear or Motor Trend magazine shoots, 
shot with a wide-angle lens (24mm), shallow depth of field to blur the background while 
keeping the car in tack-sharp focus, cinematic color grading with rich warm tones, 8K 
resolution with meticulous attention to reflections on the car's paint and glinting 
highlights on chrome accents.
```

**Why detailed prompts work better:**
- Reduces ambiguity
- Layers constraints for better control
- Uses redundant phrasing to reinforce critical concepts
- Provides visual anchors (magazine references, specific lenses)
- Specifies exactly what you want

### Negative Prompts

**Default negative prompt (SD 1.5):**
```
bad quality, blurry, low resolution, watermark, text, deformed, ugly, duplicate
```

**For photorealism:**
```
cartoon, anime, illustration, painting, sketch, 3d render, CGI, digital art, 
cel shaded, low poly, unrealistic, stylized, painterly
```

**For avoiding duplicates:**
```
multiple objects, duplicate, cloned, ghosting, mirrored, two cars, extra person, 
multiple subjects, repeated elements
```

**For perspective control (sprites, top-down):**
```
perspective, side view, diagonal, isometric, 3D, angled, tilted, depth, 
foreshortening, vanishing point, horizon line
```

### Video Prompts (Wan 2.2)

**Good structure:**
```
[action/motion], [subject], [setting], [camera movement]
```

**Example:**
```
a car driving along a coastal highway, waves crashing, drone shot following
```

**Note:** Wan 2.2 workflows typically do not use negative prompts.

---

## Error Handling

### Common Errors

| Error | Cause | Solution |
|-------|-------|----------|
| "Cannot connect to server" | ComfyUI down | Start via `scripts/start_comfyui.py` |
| "Model not found" | Missing model | Check MODEL_REGISTRY.md |
| "Invalid JSON" | Malformed workflow | Re-export from ComfyUI |
| "Input image not found" | Wrong path | Use absolute paths |
| "MinIO upload failed" | MinIO down | Check 192.168.1.215:9000 |

### Exit Codes

- **0**: Success
- **1**: Generation or runtime failure
- **2**: Configuration error (server down, missing models)

### ComfyUI Not Responding

```bash
# Check status
curl -s http://192.168.1.215:8188/system_stats

# Start ComfyUI
ssh moira "C:\\Users\\jrjen\\comfy\\.venv\\Scripts\\python.exe C:\\Users\\jrjen\\comfy-gen\\scripts\\start_comfyui.py"
```

### Model Not Found

1. Check exact filename in MODEL_REGISTRY.md
2. Verify model exists: `ssh moira "dir C:\\Users\\jrjen\\comfy\\models\\checkpoints"`
3. Model names are case-sensitive

### Best Practices

1. **Always check server availability first**
2. **Use dry-run mode for new workflows**: `--dry-run`
3. **Handle missing models gracefully**
4. **Use automatic retry for transient failures** (built-in)
5. **Clean up on cancellation**

---

## Gallery Browser

### Local Gallery Server

Browse generated images with thumbnails and metadata:

```bash
python3 scripts/gallery_server.py
# Open http://localhost:8080
```

**Features:**

**Phase 1: View Preferences**
- **Multiple View Modes**: Grid (Small/Medium/Large) and List view
- **Thumbnail Size Control**: Choose between small (200px), medium (300px), or large (400px) grid layouts
- **Advanced Search**: Search by prompt text in real-time
- **Multiple Filters**: 
  - All Images / With LoRA / Validated (CLIP > 0.9) / Favorites
  - Quality grade filter (A / B+ / C+ / D+)
- **Flexible Sorting**: Newest / Oldest / Quality Score / Name A-Z
- **Persistent Preferences**: View settings automatically saved to localStorage

**Phase 2: Quality Integration**
- **Quality Score Display**: Composite quality scores shown prominently on thumbnails
- **Color-Coded Grades**: Visual grade indicators with color-coded overlays
  - Grade A: Dark green
  - Grade B: Medium green
  - Grade C: Yellow-green
  - Grade D: Orange-red
  - Grade F: Dark red
- **Quality Filtering**: Filter images by minimum quality grade threshold
- **Quality-Based Sorting**: Sort gallery by quality scores

**Phase 3: Image Management**
- **Multi-Select**: 
  - Ctrl/Cmd + Click: Toggle individual selection
  - Shift + Click: Select range
- **Bulk Operations**:
  - Toggle favorite status for multiple images
  - Download selected images (ZIP - requires server implementation)
  - Bulk delete (requires server implementation)
- **Favorites System**: Star/favorite images for quick access
- **Action Bar**: Appears when images are selected with bulk operation buttons

**Additional Features:**
- **Full-Size Preview**: Click images to view in modal overlay
- **Lazy Loading**: Efficient loading of thumbnails for better performance
- **Responsive Design**: Optimized for desktop and tablet viewing
- **Keyboard Shortcuts**: ESC to close modal or clear selection
- **Quality Metadata**: Shows CLIP scores, technical scores, and generation parameters

### MinIO Console

Official MinIO web interface for file management:

```
http://192.168.1.215:9001
Login: minioadmin / minioadmin
```

### Direct URL Access

Images are publicly accessible:
```
http://192.168.1.215:9000/comfy-gen/<filename>.png
```

Metadata sidecars:
```
http://192.168.1.215:9000/comfy-gen/<filename>.png.json
```

### Metadata Management

**View Embedded Metadata:**

ComfyGen embeds comprehensive generation metadata directly into PNG files:

```bash
# View metadata from any generated PNG
python3 generate.py metadata show /tmp/sunset.png
```

Output shows:
- Prompt and negative prompt
- Model, VAE, and workflow used
- All generation parameters (seed, steps, CFG, sampler, etc.)
- LoRAs applied with strengths
- Quality scores (if enabled)
- Generation time and file size

**Embedded Metadata Features:**

- **Portable**: Metadata travels with the image file
- **No Sidecar Loss**: No risk of losing JSON files when moving images
- **Tool Compatible**: Readable by ExifTool, ImageMagick, and image viewers
- **CivitAI Ready**: Images can be uploaded to CivitAI with full generation info

**Disable Embedding:**

```bash
python3 generate.py --workflow ... --no-embed-metadata
```

Sidecar JSON files are always created regardless of embedding setting.

---

## See Also

- [MODEL_REGISTRY.md](MODEL_REGISTRY.md) - Available models and LoRAs
- [API_REFERENCE.md](API_REFERENCE.md) - Internal module documentation
- [ARCHITECTURE.md](ARCHITECTURE.md) - System design and workflows
