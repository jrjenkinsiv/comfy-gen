# ComfyGen Usage Guide

This comprehensive guide covers all aspects of using ComfyGen for AI-driven image and video generation.

## Table of Contents

- [Quick Start](#quick-start)
- [CLI Usage](#cli-usage)
- [MCP Server Integration](#mcp-server-integration)
- [Decision Tree](#decision-tree)
- [Workflows](#workflows)
- [Models and LoRAs](#models-and-loras)
- [Generation Parameters](#generation-parameters)
- [Prompt Engineering](#prompt-engineering)
- [Validation and Quality Control](#validation-and-quality-control)
- [Error Handling](#error-handling)
- [End-to-End Examples](#end-to-end-examples)

---

## Quick Start

### Simple Image Generation

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a red sports car on a mountain road, cinematic lighting" \
    --output /tmp/car.png
```

The image will be:
1. Generated by ComfyUI on moira
2. Saved locally to `/tmp/car.png`
3. Uploaded to MinIO at `http://192.168.1.215:9000/comfy-gen/<timestamp>_car.png`

### Viewing Generated Images

Images are viewable directly in browser:
```
http://192.168.1.215:9000/comfy-gen/<filename>.png
```

List all images:
```bash
curl -s http://192.168.1.215:9000/comfy-gen/ | grep -oP '(?<=<Key>)[^<]+'
```

---

## CLI Usage

### Basic Generation

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "your prompt here" \
    --output /tmp/output.png
```

### With LoRAs

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "anime girl portrait" \
    --lora "add-detail:0.8" \
    --output /tmp/output.png
```

### With Presets

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "detailed fantasy scene" \
    --preset high-quality \
    --output /tmp/scene.png
```

### Advanced Parameters

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "portrait of a warrior" \
    --steps 50 \
    --cfg 7.5 \
    --sampler dpmpp_2m_sde \
    --scheduler karras \
    --seed 12345 \
    --width 768 \
    --height 512 \
    --output /tmp/warrior.png
```

---

## MCP Server Integration

The Model Context Protocol (MCP) server enables AI assistants like Claude and VS Code Copilot to use ComfyGen directly.

### What is MCP?

MCP is an open protocol that allows AI assistants to:
- Access tools and functions
- Read data from various sources
- Interact with external systems

### Available Tools (25 total)

**Image Generation (2 tools)**
- `generate_image` - Text-to-image with full parameter control
- `img2img` - Transform existing images

**Video Generation (2 tools)**
- `generate_video` - Text-to-video with Wan 2.2
- `image_to_video` - Animate images to video

**Model Management (6 tools)**
- `list_models` / `list_loras` - See what's installed
- `suggest_model` / `suggest_loras` - Get recommendations
- `get_model_info` - Get detailed metadata
- `search_civitai` - Discover new models

**Gallery & History (4 tools)**
- `list_images` - Browse generated content
- `get_image_info` - See generation parameters
- `delete_image` - Clean up storage
- `get_history` - Review recent generations

**Prompt Engineering (3 tools)**
- `build_prompt` - Construct weighted prompts
- `suggest_negative` - Get negative prompts
- `analyze_prompt` - Get improvement suggestions

**Progress & Control (4 tools)**
- `get_progress` - Monitor generation jobs
- `cancel` - Stop current generation
- `get_queue` - View queued jobs
- `get_system_status` - Check GPU/VRAM/server health

**Service Management (4 tools)**
- `start_comfyui_service` - Start ComfyUI server
- `stop_comfyui_service` - Stop ComfyUI server
- `restart_comfyui_service` - Restart ComfyUI server
- `check_comfyui_service_status` - Check server status

### Setup for VS Code / Claude Desktop

Add to your MCP client configuration:

```json
{
  "mcpServers": {
    "comfy-gen": {
      "command": "python3",
      "args": ["/path/to/comfy-gen/mcp_server.py"],
      "env": {
        "COMFYUI_HOST": "http://192.168.1.215:8188",
        "MINIO_ENDPOINT": "192.168.1.215:9000",
        "MINIO_BUCKET": "comfy-gen",
        "CIVITAI_API_KEY": "${CIVITAI_API_KEY}"
      }
    }
  }
}
```

**For Claude Desktop:**
- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

**For VS Code:**
- Create `.vscode/mcp.json` in your workspace
- Or add to global VS Code settings.json

### MCP Usage Examples

#### Generate an Image
```
AI Agent: "Generate a sunset over mountains"
→ Uses generate_image tool
→ Returns: http://192.168.1.215:9000/comfy-gen/20260104_123456_output.png
```

#### Intelligent Workflow
```
AI Agent: "Create a realistic portrait and animate it"
1. Uses suggest_model(task="portrait") → Gets SD 1.5
2. Uses generate_image(...) → Creates portrait
3. Uses suggest_loras(...) → Gets motion LoRAs
4. Uses image_to_video(...) → Animates portrait
→ Returns: Video URL
```

#### Discover Models
```
AI Agent: "Find detail enhancer LoRAs for cars"
→ Uses search_civitai(query="car detail", type="lora")
→ Returns: List of models with download links
```

---

## Decision Tree

Use this flowchart to select the appropriate model and workflow:

```
START: User requests generation
    |
    ├─ Contains "video", "animation", "motion"?
    |   YES -> Is there an input image?
    |           YES -> Use Wan 2.2 I2V
    |           |      Workflow: workflows/wan22-i2v.json
    |           |      Model: wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors
    |           |      LoRA: wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
    |           |      Steps: 4, Duration: ~10 seconds
    |           |
    |           NO  -> Use Wan 2.2 T2V
    |                  Workflow: workflows/wan22-t2v.json
    |                  Model: wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors
    |                  LoRA: wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors
    |                  Steps: 4, Duration: ~10 seconds
    |
    ├─ Is there an input image to transform?
    |   YES -> Use SD 1.5 img2img
    |          Workflow: workflows/sd15-img2img.json
    |          Model: v1-5-pruned-emaonly-fp16.safetensors
    |          Denoise: 0.3-0.5 (subtle), 0.7-0.9 (creative)
    |          Duration: 10-30 seconds
    |
    NO -> Use SD 1.5 / Flux
          Workflow: workflows/flux-dev.json
          Model: v1-5-pruned-emaonly-fp16.safetensors
          Duration: 10-30 seconds

ADDITIONAL CONSIDERATIONS:
- Need quality validation? Add --validate --auto-retry
- Need specific physics/motion? Check LoRA catalog for enhancement LoRAs
- Fast generation priority? Ensure acceleration LoRAs are used (4-step)
```

### Decision Examples

**Request:** "Generate a video of a person walking"
- Contains "video" → YES
- Input image? → NO
- **Decision:** Wan 2.2 T2V, workflow: `wan22-t2v.json`

**Request:** "Transform this image to oil painting style"
- Video request? → NO
- Input image? → YES
- **Decision:** SD 1.5 img2img, workflow: `sd15-img2img.json`, denoise: 0.7

**Request:** "Create an image of a sunset"
- Video request? → NO
- Input image? → NO
- **Decision:** SD 1.5, workflow: `flux-dev.json`

**Request:** "Animate this photo with camera movement"
- Contains "animate" → YES (video)
- Input image? → YES
- **Decision:** Wan 2.2 I2V, workflow: `wan22-i2v.json`

---

## Workflows

A workflow is a JSON file that defines:
1. **What model to use** (checkpoint)
2. **What prompt to encode** (positive/negative)
3. **How to sample** (steps, CFG, sampler)
4. **What LoRAs to apply** (optional)

### Available Workflow Templates

| Template | Use Case | Notes |
|----------|----------|-------|
| `flux-dev.json` | Simple SD 1.5 images | Fast, general purpose |
| `wan22-t2v.json` | Text-to-video | Uses Wan 2.2 with 4-step LoRA |
| `wan22-i2v.json` | Image-to-video | Animates existing images |

### Basic SD 1.5 Workflow Structure

```json
{
  "3": {
    "class_type": "CheckpointLoaderSimple",
    "inputs": {
      "ckpt_name": "v1-5-pruned-emaonly-fp16.safetensors"
    }
  },
  "6": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "YOUR PROMPT HERE",
      "clip": ["3", 1]
    }
  },
  "7": {
    "class_type": "CLIPTextEncode",
    "inputs": {
      "text": "bad quality, blurry",
      "clip": ["3", 1]
    }
  }
}
```

### Creating Custom Workflows

1. Export from ComfyUI GUI
2. Save to `workflows/` with descriptive name
3. Add metadata (see docs/ARCHITECTURE.md)

---

## Models and LoRAs

### Base Models (Checkpoints)

| Model | Filename | Best For |
|-------|----------|----------|
| SD 1.5 | `v1-5-pruned-emaonly-fp16.safetensors` | General images, fast |

### Video Models

| Model | Filename | Type |
|-------|----------|------|
| Wan 2.2 T2V High | `wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors` | Text-to-video |
| Wan 2.2 T2V Low | `wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors` | Text-to-video |
| Wan 2.2 I2V High | `wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors` | Image-to-video |
| Wan 2.2 I2V Low | `wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors` | Image-to-video |

### Using LoRAs

LoRAs modify the model to achieve specific styles or effects.

**CLI Usage:**
```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "your prompt" \
    --lora "style_lora.safetensors:0.8" \
    --output /tmp/output.png
```

**Multiple LoRAs:**
```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "your prompt" \
    --lora "first_lora.safetensors:0.7" \
    --lora "second_lora.safetensors:0.5" \
    --output /tmp/output.png
```

**LoRA Presets:**
```bash
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "person walking" \
    --lora-preset "text_to_video" \
    --output /tmp/video.mp4
```

### LoRA Strength Guidelines

| Strength | Effect |
|----------|--------|
| 0.3-0.5 | Subtle influence |
| 0.6-0.8 | Moderate effect |
| 0.9-1.0 | Strong effect |
| 1.0+ | May cause artifacts |
| Negative | Inverse effect |

### Acceleration LoRAs (Reduce Steps)

| LoRA | Compatible With | Steps |
|------|----------------|-------|
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors` | Wan T2V High | 4 |
| `wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors` | Wan T2V Low | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors` | Wan I2V High | 4 |
| `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors` | Wan I2V Low | 4 |

For complete model inventory, see `docs/MODEL_REGISTRY.md`.

---

## Generation Parameters

### Parameter Reference

| Parameter | Range | Default | Effect |
|-----------|-------|---------|--------|
| `--steps` | 1-150 | 20 | Number of sampling steps. More steps = finer detail but slower. |
| `--cfg` | 1.0-20.0 | 7.0 | Classifier-free guidance scale. Higher = stricter prompt adherence. |
| `--seed` | -1 or int | random | Random seed for reproducibility. -1 for random. |
| `--width` | 64-2048 | 512 | Output width in pixels (must be divisible by 8). |
| `--height` | 64-2048 | 512 | Output height in pixels (must be divisible by 8). |
| `--sampler` | string | varies | Sampler algorithm (euler, dpmpp_2m, dpmpp_2m_sde, etc.). |
| `--scheduler` | string | normal | Noise scheduler (normal, karras, exponential). |
| `--denoise` | 0.0-1.0 | 1.0 | Denoising strength for img2img. 1.0 = full generation. |

### Generation Presets

| Preset | Steps | CFG | Sampler | Scheduler | Use Case |
|--------|-------|-----|---------|-----------|----------|
| `draft` | 10 | 5.0 | euler | normal | Quick previews, testing prompts |
| `balanced` | 20 | 7.0 | euler_ancestral | normal | Default quality/speed balance |
| `high-quality` | 50 | 7.5 | dpmpp_2m_sde | karras | Final outputs, detailed work |
| `fast` | 15 | 7.0 | dpmpp_2m | normal | Good quality with speed |
| `ultra` | 100 | 8.0 | dpmpp_2m_sde | karras | Maximum quality, very slow |

### Parameter Selection Guidelines

**Steps:**
- **10-15 steps**: Draft/preview quality, very fast
- **20-30 steps**: Standard quality, good balance
- **40-60 steps**: High quality, slower
- **80-150 steps**: Diminishing returns, use only for critical work

**CFG (Classifier-Free Guidance):**
- **1.0-4.0**: Loose interpretation, more creative/random
- **5.0-7.0**: Balanced adherence (recommended)
- **8.0-12.0**: Strict prompt following
- **13.0+**: Risk of over-saturation and artifacts

**Samplers:**
- **euler**: Fast, simple, good for drafts
- **euler_ancestral**: Adds randomness, varied outputs
- **dpmpp_2m**: Fast, high quality, good default
- **dpmpp_2m_sde**: Slower but higher quality
- **dpmpp_2m_sde_gpu**: GPU-optimized version

**Schedulers:**
- **normal**: Standard linear noise schedule
- **karras**: Better detail preservation, recommended for quality
- **exponential**: Alternative noise distribution
- **sgm_uniform**: Specific use cases

---

## Prompt Engineering

### For SD 1.5

**Good prompt structure:**
```
[subject], [style], [lighting], [quality modifiers]
```

**Example:**
```
a red Porsche 911 on a mountain road, cinematic photography, golden hour lighting, highly detailed, 8k
```

**Default negative prompt (SD 1.5):**
```
bad quality, blurry, low resolution, watermark, text, deformed, ugly, duplicate
```

**Common negative prompt additions:**

For photorealism:
```
cartoon, anime, illustration, painting, sketch, 3d render
```

For avoiding duplicates (cars, people, objects):
```
multiple objects, duplicate, cloned, ghosting, mirrored, two cars, extra person
```

For avoiding artifacts:
```
distorted, malformed, disfigured, jpeg artifacts, compression artifacts
```

For clean composition:
```
cropped, out of frame, cut off, signatures, frames, borders
```

### For Video (Wan 2.2)

**Good prompt structure:**
```
[action/motion], [subject], [setting], [camera movement]
```

**Example:**
```
a car driving along a coastal highway, waves crashing, drone shot following
```

**Note:** Wan 2.2 workflows typically do not use negative prompts. The model architecture handles quality through the positive prompt and sampling parameters.

---

## Validation and Quality Control

### Overview

ComfyGen includes automated validation to detect quality issues and optionally retry generation with adjusted prompts.

### Basic Usage

```bash
# Just validate (no retry)
python3 generate.py --workflow workflows/flux-dev.json \
    --prompt "a red Porsche 911" \
    --output /tmp/car.png \
    --validate

# Validate with auto-retry
python3 generate.py --workflow workflows/flux-dev.json \
    --prompt "(Porsche 911:2.0) single car, one car only" \
    --negative-prompt "multiple cars, duplicate" \
    --output /tmp/car.png \
    --validate --auto-retry --retry-limit 3
```

### Validation Options

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| `--validate` | Boolean | False | Run validation after generation |
| `--auto-retry` | Boolean | False | Retry if validation fails |
| `--retry-limit` | Integer | 3 | Maximum retry attempts |
| `--positive-threshold` | Float | 0.25 | Minimum CLIP score (0-1) |

### CLIP Score Interpretation

- **< 0.20**: Poor semantic match (likely wrong subject or major issues)
- **0.20-0.25**: Marginal match (may have quality issues)
- **0.25-0.35**: Acceptable match (default threshold: 0.25)
- **> 0.35**: Good semantic match

### Dependencies

```bash
pip install transformers
```

---

## Error Handling

### Common Error Scenarios

| Error | Cause | Solution |
|-------|-------|----------|
| "Cannot connect to server" | ComfyUI down | Start via `scripts/start_comfyui.py` |
| "Workflow validation failed" | Missing models | Check MODEL_REGISTRY.md, use suggested fallbacks |
| "Invalid JSON in workflow" | Malformed workflow | Re-export from ComfyUI |
| "Input image not found" | Wrong path | Verify file exists, use absolute paths |
| "Failed to upload to MinIO" | MinIO down/misconfigured | Check MinIO at 192.168.1.215:9000 |

### Exit Codes

- **0**: Success
- **1**: Generation or runtime failure
- **2**: Configuration error (server down, missing models, etc.)

### Best Practices

1. **Always check server availability first**
2. **Use dry-run mode for new workflows**: `--dry-run`
3. **Handle missing models gracefully**: Check MODEL_REGISTRY.md
4. **Use automatic retry for transient failures**: Built-in retry logic with exponential backoff
5. **Clean up on cancellation**: Use signal handlers

---

## End-to-End Examples

### Example 1: Simple Image with Validation

```bash
python3 generate.py \
    --workflow workflows/flux-dev.json \
    --prompt "a serene mountain lake at sunset, reflection in water, photorealistic, 8k" \
    --negative-prompt "blurry, low quality, distorted, watermark" \
    --output /tmp/lake.png \
    --validate \
    --auto-retry \
    --retry-limit 3 \
    --positive-threshold 0.28
```

### Example 2: Image Transformation

```bash
python3 generate.py \
    --workflow workflows/sd15-img2img.json \
    --input-image http://192.168.1.215:9000/comfy-gen/original_photo.png \
    --resize 512x512 \
    --crop cover \
    --denoise 0.7 \
    --prompt "watercolor painting, soft colors, artistic, impressionist style" \
    --negative-prompt "photograph, realistic, sharp, detailed" \
    --output /tmp/artistic.png
```

### Example 3: Video Generation

```bash
python3 generate.py \
    --workflow workflows/wan22-t2v.json \
    --prompt "a drone shot flying over a coastal highway, waves crashing, sunset lighting, cinematic" \
    --output /tmp/coastal_video.mp4
```

**Wan 2.2 Video Specifications:**
- **Resolution:** 848x480 pixels
- **Frame Count:** 81 frames
- **Frame Rate:** 8 fps
- **Duration:** ~10 seconds
- **Steps:** 4 (with acceleration LoRA)
- **CFG:** 1.0 (recommended with 4-step LoRA)

### Example 4: Dry-Run Validation

```bash
python3 generate.py \
    --workflow workflows/custom_workflow.json \
    --dry-run
```

### Example 5: Batch Generation

```bash
for workflow in workflows/*.json; do
    echo "Validating $workflow..."
    python3 generate.py --workflow "$workflow" --dry-run || echo "FAILED: $workflow"
done
```

---

## Summary Checklist

**Before generating:**
- [ ] ComfyUI server is running
- [ ] Correct workflow selected based on decision tree
- [ ] Models exist (use `--dry-run`)
- [ ] Prompts are well-structured
- [ ] Input images are prepared if needed
- [ ] Validation enabled if quality is critical
- [ ] Output path is writable
- [ ] MinIO is accessible for upload

**After generation:**
- [ ] Check exit code (0 = success, 1 = failure, 2 = config error)
- [ ] Verify MinIO URL is accessible
- [ ] Review validation scores if enabled
- [ ] Clean up temporary files

---

## See Also

- [API_REFERENCE.md](API_REFERENCE.md) - Internal module and function documentation
- [MODEL_REGISTRY.md](MODEL_REGISTRY.md) - Complete model inventory
- [ARCHITECTURE.md](ARCHITECTURE.md) - System architecture and workflows
